name: scrape
on:
  workflow_dispatch:
  schedule:
    # - cron: '0 0,6,12,18 * * *'
jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Check out this repo
        uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'
      - name: Generate requirements from pipenv file
        uses: VaultVulp/action-pipenv@v2.0.1
        with:
          command: requirements > requirements.txt
      - name: Install requiremeents
        run: pip install -r requirements.txt
      - name: Run the scraping script
        run: python main.py > readme.md
      - name: Extract average
        run: "echo `date +'%Y-%m-%d %H:%M'`: `cat readme.md | tail -1` >> average.txt"
      - name: Commit and push if content changed
        run: |-
          git config user.name "Automated"
          git config user.email "actions@users.noreply.github.com"
          git add readme.md || exit 0
          git add average.txt || exit 0
          timestamp=$(date +'%Y-%m-%d %H:%M')
          git commit -m "Latest data: ${timestamp}" || exit 0
          git push
